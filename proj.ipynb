{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sorting', 'hat', 'door', 'swung', 'open', 'once', 'tall', 'witch', 'robes', 'stood', 'there', 'she', 'had', 'very', 'face', 'and', 'harry', 'first', 'thought', 'was', 'that', 'this', 'not', 'someone', 'years', 'professor', 'mcgonagall', 'said', 'hagrid', 'you', 'will', 'take', 'them', 'from', 'here', 'pulled', 'wide', 'entrance', 'hall', 'big', 'could', 'have', 'whole', 'dursleys', 'house', 'stone', 'walls', 'were', 'with', 'like', 'ones', 'gringotts', 'ceiling', 'too', 'high', 'make', 'out', 'marble', 'staircase', 'they', 'followed', 'across', 'floor', 'hear', 'hundreds', 'right', 'rest', 'school', 'must', 'already', 'but', 'showed', 'into', 'small', 'empty', 'chamber', 'off', 'standing', 'together', 'than', 'would', 'usually', 'done', 'about', 'hogwarts', 'before', 'your', 'great', 'houses', 'important', 'because', 'while', 'are', 'something', 'family', 'classes', 'sleep', 'dormitory', 'free', 'time', 'common', 'room', 'four', 'called', 'gryffindor', 'hufflepuff', 'ravenclaw', 'slytherin', 'each', 'has', 'its', 'own', 'wizards', 'points', 'any', 'rule', 'breaking', 'lose', 'end', 'year', 'most', 'cup', 'hope', 'place', 'few', 'minutes', 'front', 'all', 'much', 'can', 'waiting', 'her', 'eyes', 'for', 'moment', 'neville', 'cloak', 'which', 'under', 'his', 'left', 'ear', 'ron', 'nose', 'tried', 'hair', 'when', 'ready', 'please', 'wait', 'quietly', 'how', 'exactly', 'sort', 'asked', 'some', 'think', 'fred', 'lot', 'heart', 'gave', 'horrible', 'did', \"n't\", 'know', 'magic', 'yet', 'what', 'earth', 'arrived', 'looked', 'around', 'saw', 'everyone', 'else', 'one', 'talking', 'except', 'hermione', 'granger', 'who', 'fast', 'wondering', 'need', 'hard', 'listen', 'never', 'been', 'more', 'nervous', 'even', 'home', 'saying', 'turned', 'teachers', 'blue', 'kept', 'second', 'now', 'come', 'back', 'lead', 'him', 'then', 'happened', 'made', 'jump', 'foot', 'air', 'several', 'people', 'behind', 'screamed', 'gasped', 'twenty', 'ghosts', 'just', 'through', 'wall', 'another', 'hardly', 'seemed', 'fat', 'little', 'forget', 'say', 'give', 'chance', 'dear', 'given', 'peeves', 'bad', 'name', 'really', 'ghost', 'doing', 'wearing', 'suddenly', 'noticed', 'new', 'students', 'smiling', 'suppose', 'see', 'old', 'move', 'along', 'voice', 'start', 'away', 'opposite', 'line', 'told', 'follow', 'feeling', 'though', 'legs', 'got', 'boy', 'walked', 'pair', 'doors', 'such', 'strange', 'thousands', 'floating', 'midair', 'over', 'long', 'tables', 'where', 'sitting', 'these', 'golden', 'plates', 'top', 'table', 'came', 'other', 'faces', 'staring', 'silver', 'black', 'heard', 'look', 'outside', 'read', 'believe', 'quickly', 'down', 'again', 'stool', 'put', 'pointed', 'wizard', 'let', 'maybe', 'try', 'get', 'thing', 'noticing', 'stared', 'seconds', 'silence', 'near', 'opened', 'mouth', 'began', 'may', 'eat', 'find', 'keep', 'nothing', 'head', 'tell', 'might', 'their', 'set', 'gryffindors', 'apart', 'those', 'true', 'mind', 'always', 'perhaps', 'friends', 'use', 'safe', 'hands', 'none', 'thinking', 'burst', 'finished', 'became', 'quite', 'still', 'whispered', 'going', 'troll', 'smiled', 'yes', 'trying', 'better', 'having', 'without', 'watching', 'feel', 'only', 'felt', 'bit', 'stepped', 'forward', 'holding', 'call', 'sit', 'fell', 'sat', 'shouted', 'clapped', 'went', 'waving', 'next', 'joined', 'far', 'brothers', 'after', 'being', 'picked', 'teams', 'during', 'last', 'good', 'wanted', 'liked', 'others', 'took', 'finnigan', 'seamus', 'almost', 'minute', 'ran', 'ages', 'until', 'obviously', 'train', 'longbottom', 'way', 'finally', 'laughter', 'malfoy', 'barely', 'crabbe', 'goyle', 'looking', 'pleased', 'himself', 'many', 'girls', 'potter', 'broke', 'dropped', 'full', 'inside', 'difficult', 'either', 'nice', 'yourself', 'interesting', 'sure', 'help', 'well', 'word', 'toward', 'getting', 'percy', 'prefect', 'shook', 'hand', 'weasley', 'twins', 'yelled', 'seen', 'arm', 'giving', 'sudden', 'properly', 'caught', 'eye', 'large', 'gold', 'dumbledore', 'card', 'gotten', 'spotted', 'quirrell', 'man', 'cauldron', 'turban', 'dean', 'turn', 'green', 'crossed', 'fingers', 'later', 'loudly', 'excellent', 'rolled', 'realized', 'feet', 'arms', 'our', 'words', 'whether', 'mad', 'best', 'piled', 'food', 'things', 'roast', 'reason', 'allowed', 'taken', 'anything', 'everything', 'does', 'nearly', 'five', 'hundred', 'course', 'miss', 'sir', 'tower', 'headless', 'seized', 'neck', 'onto', 'shoulder', 'win', 'gone', 'slytherins', 'six', 'row', 'bloody', 'baron', 'blood', 'covered', 'appeared', 'every', 'talk', 'muggle', 'nasty', 'brought', 'uncle', 'catch', 'pushed', 'round', 'dinner', 'hanging', 'upstairs', 'window', 'should', 'enough', 'side', 'lessons', 'learn', 'particularly', 'turning', 'supposed', 'warm', 'teacher', 'past', 'straight', 'shot', 'wonder', 'snape', 'potions', 'want', 'knows', 'job', 'dark', 'arts', 'watched', 'disappeared', 'note', 'forest', 'grounds', 'forbidden', 'remember', 'direction', 'also', 'mr', 'filch', 'used', 'between', 'corridors', 'quidditch', 'held', 'week', 'anyone', 'playing', 'madam', 'hooch', 'third', 'corridor', 'muttered', 'why', 'somewhere', 'dangerous', 'bed', 'wand', 'fly', 'flew', 'rose', 'above', 'twisted', 'teach', 'knees', 'heads', 'dead', 'worth', 'knowing', 'different', 'times', 'passed', 'climbed', 'walking', 'ahead', 'started', 'themselves', 'raised', 'show', 'loud', 'sound', 'mean', 'stuck', 'armor', 'watch', 'hung', 'portrait', 'hole', 'needed', 'leg', 'found', 'red', 'ask', 'asleep', 'telling', 'pull', 'laughing', 'cold', 'light', 'woke', 'shaking', 'day', 'glasses', 'visit', 'point', 'two', 'locked', 'met', 'late', 'class', 'invisible', 'worse', 'possible', 'managed', 'wrong', 'morning', 'lost', 'break', 'lock', 'dungeons', 'mrs', 'norris', 'alone', 'knew', 'secret', 'funny', 'study', 'night', 'midnight', 'three', 'castle', 'fire', 'flitwick', 'charms', 'stand', 'pile', 'books', 'reached', 'sight', 'leave', 'taking', 'match', 'lesson', 'against', 'classroom', 'coming', 'rid', 'smell', 'idea', 'breakfast', 'able', 'stopped', 'huge', 'homework', 'owls', 'hedwig', 'however', 'tea', 'send', 'answer', 'sent', 'lucky', 'glass', 'spoke', 'keeping', 'expect', 'understand', 'seat', 'added', 'tut', 'clearly', 'ignored', 'book', 'thousand', 'snapped', 'potion', 'save', 'same', 'goes', 'noise', 'filled', 'moaned', 'angry', 'working', 'hour', 'low', 'george', 'meet', 'wooden', 'knocked', 'fang', 'crack', 'hang', 'corner', 'ears', 'spent', 'half', 'life', 'yer', 'ter', 'charlie', 'wondered', 'work', 'dragons', 'lying', 'afternoon', 'seven', 'package', 'notice', 'flying', 'broomstick', 'anyway', 'bet', 'hit', 'broom', 'soccer', 'game', 'ball', 'team', 'players', 'both', 'ground', 'stupid', 'library', 'letter', 'since', 'owl', 'size', 'white', 'remembrall', 'forgotten', 'jumped', 'hoping', 'trouble', 'hurried', 'grass', 'marched', 'flat', 'brooms', 'gray', 'moved', 'twelve', 'lay', 'higher', 'shut', 'tree', 'branches', 'grabbed', 'easy', 'knock', 'yeah', 'leaned', 'threw', 'fall', 'dive', 'running', 'furiously', 'expelled', 'weeks', 'ten', 'stay', 'excuse', 'wood', 'busy', 'oliver', 'seeker', 'seem', 'ever', 'nimbus', 'speak', 'training', 'father', 'won', 'lee', 'jordan', 'tonight', 'matter', 'trophy', 'wandering', 'listening', 'evening', 'curse', 'luck', 'crept', 'shadows', 'lamp', 'stop', 'lady', 'nearer', 'snout', 'pomfrey', 'windows', 'run', 'speaking', 'probably', 'hiding', 'following', 'footsteps', 'dog', 'breath', 'guarding', 'halloween', 'underneath', 'usual', 'parcel', 'field', 'wanting', 'thanks', 'rules', 'stairs', 'play', 'eating', 'stands', 'hoops', 'goal', 'posts', 'crate', 'practice', 'balls', 'chasers', 'bright', 'quaffle', 'score', 'keeper', 'box', 'club', 'bludgers', 'bludger', 'weasleys', 'snitch', 'extra', 'making', 'friend', 'close', 'crowd', 'slipped', 'thick', 'slowly', 'terrible', 'hurt', 'saturday', 'decided', 'steal', 'binoculars', 'locker', 'chaser', 'agreed', 'marcus', 'flint', 'sorry', 'possession', 'hut', 'outta', 'reflection', 'wild', 'fluffy', 'yeh', 'don', 'nicolas', 'flamel', 'mirror', 'erised', 'christmas', 'holidays', 'restricted', 'section', 'studying', 'parents', 'chess', 'sweater', 'invisibility', 'shows', 'dragon', 'sorcerer', 'norbert', 'egg']\n",
      "869\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "with open('episodes\\\\HP1.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab_list = Counter([re.sub('[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~0-9]', '', i) for i in word_tokenize(text.lower()) if i[0].isalpha() and len(i)>2])\n",
    "vocab_list = dict(vocab_list)\n",
    "for i in list(vocab_list.keys()):\n",
    "    if vocab_list[i] < 5:\n",
    "        del vocab_list[i]\n",
    "vocab_list = list(vocab_list.keys())\n",
    "print(vocab_list)\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "\n",
    "#print(vocab)\n",
    "#print(vocab_to_int)\n",
    "#print(int_to_vocab)\n",
    "\n",
    "#encoded contains the entire text, encoded character-wise. Example: HARRY: 29 56 ...etc where 29 is H and 56 is A\n",
    "#print(encoded)\n",
    "\n",
    "def get_batches(arr, batch_size, n_steps):\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr)//chars_per_batch\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y_temp = arr[:, n+1:n+n_steps+1]\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        yield x, y\n",
    "\n",
    "\n",
    "#batches = get_batches(encoded, 10, 50)\n",
    "#x,y = next(batches)\n",
    "\n",
    "#print(x,y)\n",
    "print(len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout'''\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers'''\n",
    "\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, lstm_size=128, num_layers=2, learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719BBA9188>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719BBA9188>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719BBA9188>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719BBA9188>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A411F88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A411F88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A411F88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A411F88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A418288>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A418288>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A418288>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001708A418288>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch: 4/250...  Training Step: 50...  Training loss: 3.1285...  0.0700 sec/batch\n",
      "Epoch: 8/250...  Training Step: 100...  Training loss: 3.0435...  0.1000 sec/batch\n",
      "Epoch: 11/250...  Training Step: 150...  Training loss: 3.0286...  0.0980 sec/batch\n",
      "Epoch: 15/250...  Training Step: 200...  Training loss: 3.0154...  0.1000 sec/batch\n",
      "Epoch: 18/250...  Training Step: 250...  Training loss: 2.9582...  0.1010 sec/batch\n",
      "Epoch: 22/250...  Training Step: 300...  Training loss: 2.9393...  0.1000 sec/batch\n",
      "Epoch: 25/250...  Training Step: 350...  Training loss: 2.9115...  0.0970 sec/batch\n",
      "Epoch: 29/250...  Training Step: 400...  Training loss: 2.8806...  0.0730 sec/batch\n",
      "Epoch: 33/250...  Training Step: 450...  Training loss: 2.7704...  0.0780 sec/batch\n",
      "Epoch: 36/250...  Training Step: 500...  Training loss: 2.6643...  0.0730 sec/batch\n",
      "Epoch: 40/250...  Training Step: 550...  Training loss: 2.6038...  0.0980 sec/batch\n",
      "Epoch: 43/250...  Training Step: 600...  Training loss: 2.4949...  0.0990 sec/batch\n",
      "Epoch: 47/250...  Training Step: 650...  Training loss: 2.4517...  0.2010 sec/batch\n",
      "Epoch: 50/250...  Training Step: 700...  Training loss: 2.4424...  0.1000 sec/batch\n",
      "Epoch: 54/250...  Training Step: 750...  Training loss: 2.3481...  0.1000 sec/batch\n",
      "Epoch: 58/250...  Training Step: 800...  Training loss: 2.3221...  0.1010 sec/batch\n",
      "Epoch: 61/250...  Training Step: 850...  Training loss: 2.2786...  0.0760 sec/batch\n",
      "Epoch: 65/250...  Training Step: 900...  Training loss: 2.2609...  0.1010 sec/batch\n",
      "Epoch: 68/250...  Training Step: 950...  Training loss: 2.2340...  0.1020 sec/batch\n",
      "Epoch: 72/250...  Training Step: 1000...  Training loss: 2.2123...  0.1000 sec/batch\n",
      "Epoch: 75/250...  Training Step: 1050...  Training loss: 2.2367...  0.1000 sec/batch\n",
      "Epoch: 79/250...  Training Step: 1100...  Training loss: 2.1574...  0.1590 sec/batch\n",
      "Epoch: 83/250...  Training Step: 1150...  Training loss: 2.1552...  0.1010 sec/batch\n",
      "Epoch: 86/250...  Training Step: 1200...  Training loss: 2.1331...  0.1000 sec/batch\n",
      "Epoch: 90/250...  Training Step: 1250...  Training loss: 2.1457...  0.1010 sec/batch\n",
      "Epoch: 93/250...  Training Step: 1300...  Training loss: 2.1117...  0.1010 sec/batch\n",
      "Epoch: 97/250...  Training Step: 1350...  Training loss: 2.0908...  0.0990 sec/batch\n",
      "Epoch: 100/250...  Training Step: 1400...  Training loss: 2.1545...  0.0770 sec/batch\n",
      "Epoch: 104/250...  Training Step: 1450...  Training loss: 2.0755...  0.0980 sec/batch\n",
      "Epoch: 108/250...  Training Step: 1500...  Training loss: 2.0569...  0.1000 sec/batch\n",
      "Epoch: 111/250...  Training Step: 1550...  Training loss: 2.0378...  0.1000 sec/batch\n",
      "Epoch: 115/250...  Training Step: 1600...  Training loss: 2.0652...  0.1000 sec/batch\n",
      "Epoch: 118/250...  Training Step: 1650...  Training loss: 2.0214...  0.1000 sec/batch\n",
      "Epoch: 122/250...  Training Step: 1700...  Training loss: 2.0251...  0.0990 sec/batch\n",
      "Epoch: 125/250...  Training Step: 1750...  Training loss: 2.0725...  0.1280 sec/batch\n",
      "Epoch: 129/250...  Training Step: 1800...  Training loss: 1.9976...  0.1250 sec/batch\n",
      "Epoch: 133/250...  Training Step: 1850...  Training loss: 2.0160...  0.1020 sec/batch\n",
      "Epoch: 136/250...  Training Step: 1900...  Training loss: 1.9830...  0.1010 sec/batch\n",
      "Epoch: 140/250...  Training Step: 1950...  Training loss: 2.0014...  0.1010 sec/batch\n",
      "Epoch: 143/250...  Training Step: 2000...  Training loss: 2.0101...  0.1040 sec/batch\n",
      "Epoch: 147/250...  Training Step: 2050...  Training loss: 1.9831...  0.0740 sec/batch\n",
      "Epoch: 150/250...  Training Step: 2100...  Training loss: 2.0316...  0.0760 sec/batch\n",
      "Epoch: 154/250...  Training Step: 2150...  Training loss: 1.9664...  0.1280 sec/batch\n",
      "Epoch: 158/250...  Training Step: 2200...  Training loss: 1.9677...  0.0990 sec/batch\n",
      "Epoch: 161/250...  Training Step: 2250...  Training loss: 1.9401...  0.1020 sec/batch\n",
      "Epoch: 165/250...  Training Step: 2300...  Training loss: 1.9770...  0.0970 sec/batch\n",
      "Epoch: 168/250...  Training Step: 2350...  Training loss: 1.9580...  0.1030 sec/batch\n",
      "Epoch: 172/250...  Training Step: 2400...  Training loss: 1.9463...  0.1320 sec/batch\n",
      "Epoch: 175/250...  Training Step: 2450...  Training loss: 2.0036...  0.0670 sec/batch\n",
      "Epoch: 179/250...  Training Step: 2500...  Training loss: 1.9321...  0.0990 sec/batch\n",
      "Epoch: 183/250...  Training Step: 2550...  Training loss: 1.9511...  0.1000 sec/batch\n",
      "Epoch: 186/250...  Training Step: 2600...  Training loss: 1.9112...  0.1010 sec/batch\n",
      "Epoch: 190/250...  Training Step: 2650...  Training loss: 1.9432...  0.1000 sec/batch\n",
      "Epoch: 193/250...  Training Step: 2700...  Training loss: 1.9172...  0.1020 sec/batch\n",
      "Epoch: 197/250...  Training Step: 2750...  Training loss: 1.9241...  0.1020 sec/batch\n",
      "Epoch: 200/250...  Training Step: 2800...  Training loss: 1.9799...  0.1000 sec/batch\n",
      "Epoch: 204/250...  Training Step: 2850...  Training loss: 1.8903...  0.0730 sec/batch\n",
      "Epoch: 208/250...  Training Step: 2900...  Training loss: 1.9050...  0.1020 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211/250...  Training Step: 2950...  Training loss: 1.8815...  0.0980 sec/batch\n",
      "Epoch: 215/250...  Training Step: 3000...  Training loss: 1.9022...  0.0990 sec/batch\n",
      "Epoch: 218/250...  Training Step: 3050...  Training loss: 1.9056...  0.1000 sec/batch\n",
      "Epoch: 222/250...  Training Step: 3100...  Training loss: 1.8860...  0.0710 sec/batch\n",
      "Epoch: 225/250...  Training Step: 3150...  Training loss: 1.9406...  0.1020 sec/batch\n",
      "Epoch: 229/250...  Training Step: 3200...  Training loss: 1.8854...  0.1000 sec/batch\n",
      "Epoch: 233/250...  Training Step: 3250...  Training loss: 1.8885...  0.1010 sec/batch\n",
      "Epoch: 236/250...  Training Step: 3300...  Training loss: 1.8586...  0.0990 sec/batch\n",
      "Epoch: 240/250...  Training Step: 3350...  Training loss: 1.8829...  0.0720 sec/batch\n",
      "Epoch: 243/250...  Training Step: 3400...  Training loss: 1.8739...  0.0990 sec/batch\n",
      "Epoch: 247/250...  Training Step: 3450...  Training loss: 1.8780...  0.1000 sec/batch\n",
      "Epoch: 250/250...  Training Step: 3500...  Training loss: 1.9332...  0.1020 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "print_every_n = 50\n",
    "save_every_n = 200\n",
    "batch_size = 256\n",
    "num_steps = 50\n",
    "lstm_size = 128\n",
    "num_layers = 2\n",
    "learning_rate =0.1\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,lstm_size=lstm_size, num_layers=num_layers, learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x, model.targets: y, model.keep_prob: 0.6, model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, model.final_state, model.optimizer], feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i3500_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2600_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2800_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3200_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3400_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3500_l128.ckpt\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i3500_l128.ckpt'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistDP(str1, str2, m, n): \n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1): \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    \n",
    "            elif str1[i-1] == str2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],dp[i-1][j],dp[i-1][j-1])    \n",
    "    return dp[m][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(word1,word2):\n",
    "    return len(set(word1).intersection(set(word2))) / len(set(word1).union(set(word2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strcmp(word1,word2):\n",
    "    return abs(len(word1) - len(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_wrapper(word):\n",
    "\n",
    "    l1=[]\n",
    "    word = re.sub('[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~0-9]', '', word)\n",
    "    for i in vocab_list:\n",
    "        edit_dist = editDistDP(i,word,len(i),len(word))\n",
    "        l1.append((strcmp(i,word),edit_dist,jaccard_similarity(i,word),i,word))\n",
    "    return sorted([i for i in l1 if i[0]<3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719B3491C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719B3491C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719B3491C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001719B3491C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719D4C82C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719D4C82C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719D4C82C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719D4C82C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719DD01648>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719DD01648>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719DD01648>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001719DD01648>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\i3500_l128.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 10000, lstm_size, len(vocab), prime=\"Invisibility Cloak on top of the tower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fp = open(\"expected_output9\",\"w\")\n",
    "fp.write(samp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "backupsamp = copy.deepcopy(samp)\n",
    "backupsamp = word_tokenize(backupsamp)\n",
    "for i in range(len(backupsamp)):\n",
    "    if len(backupsamp[i]) >2 and backupsamp[i].lower() not in vocab_list and len(backupsamp[i].lower())>1:\n",
    "        backupsamp[i] = min_edit_wrapper(backupsamp[i].lower())[0][-2]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "backupsamp2 = untokenize(backupsamp)\n",
    "fp = open(\"output9\",\"w\")\n",
    "fp.write(backupsamp2)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
