{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('episodes\\\\book.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "\n",
    "#print(vocab)\n",
    "#print(vocab_to_int)\n",
    "#print(int_to_vocab)\n",
    "\n",
    "#encoded contains the entire text, encoded character-wise. Example: MONICA: 29 56 ...etc where 29 is M and 56 is O\n",
    "#print(encoded)\n",
    "\n",
    "def get_batches(arr, batch_size, n_steps):\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "    n_batches = len(arr)//chars_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y_temp = arr[:, n+1:n+n_steps+1]\n",
    "        \n",
    "        # For the very last batch, y will be one character short at the end of \n",
    "        # the sequences which breaks things. To get around this, I'll make an \n",
    "        # array of the appropriate size first, of all zeros, then add the targets.\n",
    "        # This will introduce a small artifact in the last batch, but it won't matter.\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        yield x, y\n",
    "\n",
    "\n",
    "#batches = get_batches(encoded, 10, 50)\n",
    "#x,y = next(batches)\n",
    "\n",
    "#print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout'''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers'''\n",
    "    \n",
    "    #Build the LSTM Cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-ee4ba377ed92>:10: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-ee4ba377ed92>:18: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-7801db356943>:27: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000237121B0C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000237121B0C08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000237121B0C08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000237121B0C08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000023710AEFE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000023710AEFE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000023710AEFE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000023710AEFE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000237121AEF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000237121AEF08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000237121AEF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000237121AEF08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-5-6d5964af2bdd>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch: 1/100...  Training Step: 50...  Training loss: 3.2025...  0.1360 sec/batch\n",
      "Epoch: 1/100...  Training Step: 100...  Training loss: 3.1979...  0.1330 sec/batch\n",
      "Epoch: 1/100...  Training Step: 150...  Training loss: 3.1118...  0.1020 sec/batch\n",
      "Epoch: 1/100...  Training Step: 200...  Training loss: 2.9290...  0.1310 sec/batch\n",
      "Epoch: 1/100...  Training Step: 250...  Training loss: 2.7511...  0.0990 sec/batch\n",
      "Epoch: 2/100...  Training Step: 300...  Training loss: 2.6593...  0.1010 sec/batch\n",
      "Epoch: 2/100...  Training Step: 350...  Training loss: 2.5855...  0.1380 sec/batch\n",
      "Epoch: 2/100...  Training Step: 400...  Training loss: 2.5277...  0.0980 sec/batch\n",
      "Epoch: 2/100...  Training Step: 450...  Training loss: 2.5109...  0.0710 sec/batch\n",
      "Epoch: 2/100...  Training Step: 500...  Training loss: 2.4433...  0.1330 sec/batch\n",
      "Epoch: 2/100...  Training Step: 550...  Training loss: 2.4644...  0.1970 sec/batch\n",
      "Epoch: 3/100...  Training Step: 600...  Training loss: 2.4012...  0.2000 sec/batch\n",
      "Epoch: 3/100...  Training Step: 650...  Training loss: 2.3935...  0.1030 sec/batch\n",
      "Epoch: 3/100...  Training Step: 700...  Training loss: 2.3828...  0.2030 sec/batch\n",
      "Epoch: 3/100...  Training Step: 750...  Training loss: 2.3062...  0.1000 sec/batch\n",
      "Epoch: 3/100...  Training Step: 800...  Training loss: 2.3183...  0.1000 sec/batch\n",
      "Epoch: 3/100...  Training Step: 850...  Training loss: 2.3112...  0.1380 sec/batch\n",
      "Epoch: 4/100...  Training Step: 900...  Training loss: 2.2732...  0.1010 sec/batch\n",
      "Epoch: 4/100...  Training Step: 950...  Training loss: 2.2735...  0.1010 sec/batch\n",
      "Epoch: 4/100...  Training Step: 1000...  Training loss: 2.2486...  0.0620 sec/batch\n",
      "Epoch: 4/100...  Training Step: 1050...  Training loss: 2.2015...  0.1000 sec/batch\n",
      "Epoch: 4/100...  Training Step: 1100...  Training loss: 2.2069...  0.0990 sec/batch\n",
      "Epoch: 4/100...  Training Step: 1150...  Training loss: 2.1921...  0.2000 sec/batch\n",
      "Epoch: 5/100...  Training Step: 1200...  Training loss: 2.1915...  0.1010 sec/batch\n",
      "Epoch: 5/100...  Training Step: 1250...  Training loss: 2.2249...  0.0690 sec/batch\n",
      "Epoch: 5/100...  Training Step: 1300...  Training loss: 2.1475...  0.0990 sec/batch\n",
      "Epoch: 5/100...  Training Step: 1350...  Training loss: 2.1889...  0.1000 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100...  Training Step: 1400...  Training loss: 2.2070...  0.1000 sec/batch\n",
      "Epoch: 5/100...  Training Step: 1450...  Training loss: 2.1696...  0.0670 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1500...  Training loss: 2.1521...  0.1330 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1550...  Training loss: 2.1733...  0.1340 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1600...  Training loss: 2.1071...  0.1000 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1650...  Training loss: 2.1230...  0.1000 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1700...  Training loss: 2.1198...  0.1000 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1750...  Training loss: 2.1302...  0.0990 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1800...  Training loss: 2.0862...  0.1010 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1850...  Training loss: 2.1384...  0.0990 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1900...  Training loss: 2.0717...  0.1000 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1950...  Training loss: 2.0630...  0.1590 sec/batch\n",
      "Epoch: 7/100...  Training Step: 2000...  Training loss: 2.0248...  0.0990 sec/batch\n",
      "Epoch: 7/100...  Training Step: 2050...  Training loss: 2.0726...  0.1000 sec/batch\n",
      "Epoch: 8/100...  Training Step: 2100...  Training loss: 2.0630...  0.1010 sec/batch\n",
      "Epoch: 8/100...  Training Step: 2150...  Training loss: 2.0747...  0.1018 sec/batch\n",
      "Epoch: 8/100...  Training Step: 2200...  Training loss: 2.0108...  0.1000 sec/batch\n",
      "Epoch: 8/100...  Training Step: 2250...  Training loss: 2.0500...  0.1360 sec/batch\n",
      "Epoch: 8/100...  Training Step: 2300...  Training loss: 2.0380...  0.0620 sec/batch\n",
      "Epoch: 8/100...  Training Step: 2350...  Training loss: 2.0183...  0.1000 sec/batch\n",
      "Epoch: 9/100...  Training Step: 2400...  Training loss: 2.0087...  0.1010 sec/batch\n",
      "Epoch: 9/100...  Training Step: 2450...  Training loss: 1.9889...  0.1750 sec/batch\n",
      "Epoch: 9/100...  Training Step: 2500...  Training loss: 2.0199...  0.1220 sec/batch\n",
      "Epoch: 9/100...  Training Step: 2550...  Training loss: 1.9861...  0.1000 sec/batch\n",
      "Epoch: 9/100...  Training Step: 2600...  Training loss: 1.9546...  0.0990 sec/batch\n",
      "Epoch: 9/100...  Training Step: 2650...  Training loss: 1.9937...  0.0660 sec/batch\n",
      "Epoch: 10/100...  Training Step: 2700...  Training loss: 1.9570...  0.0660 sec/batch\n",
      "Epoch: 10/100...  Training Step: 2750...  Training loss: 1.9660...  0.0660 sec/batch\n",
      "Epoch: 10/100...  Training Step: 2800...  Training loss: 1.9552...  0.1310 sec/batch\n",
      "Epoch: 10/100...  Training Step: 2850...  Training loss: 1.9554...  0.1360 sec/batch\n",
      "Epoch: 10/100...  Training Step: 2900...  Training loss: 1.9495...  0.1000 sec/batch\n",
      "Epoch: 10/100...  Training Step: 2950...  Training loss: 2.0232...  0.1000 sec/batch\n",
      "Epoch: 11/100...  Training Step: 3000...  Training loss: 1.9532...  0.1010 sec/batch\n",
      "Epoch: 11/100...  Training Step: 3050...  Training loss: 1.9858...  0.1000 sec/batch\n",
      "Epoch: 11/100...  Training Step: 3100...  Training loss: 1.9276...  0.0990 sec/batch\n",
      "Epoch: 11/100...  Training Step: 3150...  Training loss: 1.8855...  0.0650 sec/batch\n",
      "Epoch: 11/100...  Training Step: 3200...  Training loss: 1.8954...  0.1350 sec/batch\n",
      "Epoch: 12/100...  Training Step: 3250...  Training loss: 1.9225...  0.0660 sec/batch\n",
      "Epoch: 12/100...  Training Step: 3300...  Training loss: 1.9092...  0.1000 sec/batch\n",
      "Epoch: 12/100...  Training Step: 3350...  Training loss: 1.9445...  0.1340 sec/batch\n",
      "Epoch: 12/100...  Training Step: 3400...  Training loss: 1.9218...  0.0650 sec/batch\n",
      "Epoch: 12/100...  Training Step: 3450...  Training loss: 1.9234...  0.0810 sec/batch\n",
      "Epoch: 12/100...  Training Step: 3500...  Training loss: 1.9431...  0.1000 sec/batch\n",
      "Epoch: 13/100...  Training Step: 3550...  Training loss: 1.8797...  0.1000 sec/batch\n",
      "Epoch: 13/100...  Training Step: 3600...  Training loss: 1.9240...  0.1000 sec/batch\n",
      "Epoch: 13/100...  Training Step: 3650...  Training loss: 1.9256...  0.0660 sec/batch\n",
      "Epoch: 13/100...  Training Step: 3700...  Training loss: 1.8711...  0.1000 sec/batch\n",
      "Epoch: 13/100...  Training Step: 3750...  Training loss: 1.8762...  0.0999 sec/batch\n",
      "Epoch: 13/100...  Training Step: 3800...  Training loss: 1.8991...  0.1360 sec/batch\n",
      "Epoch: 14/100...  Training Step: 3850...  Training loss: 1.8743...  0.1020 sec/batch\n",
      "Epoch: 14/100...  Training Step: 3900...  Training loss: 1.8705...  0.1010 sec/batch\n",
      "Epoch: 14/100...  Training Step: 3950...  Training loss: 1.8425...  0.0990 sec/batch\n",
      "Epoch: 14/100...  Training Step: 4000...  Training loss: 1.8446...  0.1570 sec/batch\n",
      "Epoch: 14/100...  Training Step: 4050...  Training loss: 1.8587...  0.0640 sec/batch\n",
      "Epoch: 14/100...  Training Step: 4100...  Training loss: 1.8547...  0.1000 sec/batch\n",
      "Epoch: 15/100...  Training Step: 4150...  Training loss: 1.8521...  0.1020 sec/batch\n",
      "Epoch: 15/100...  Training Step: 4200...  Training loss: 1.8837...  0.0990 sec/batch\n",
      "Epoch: 15/100...  Training Step: 4250...  Training loss: 1.8200...  0.0650 sec/batch\n",
      "Epoch: 15/100...  Training Step: 4300...  Training loss: 1.8680...  0.0660 sec/batch\n",
      "Epoch: 15/100...  Training Step: 4350...  Training loss: 1.8516...  0.0650 sec/batch\n",
      "Epoch: 15/100...  Training Step: 4400...  Training loss: 1.8447...  0.0670 sec/batch\n",
      "Epoch: 16/100...  Training Step: 4450...  Training loss: 1.8382...  0.1010 sec/batch\n",
      "Epoch: 16/100...  Training Step: 4500...  Training loss: 1.8713...  0.0990 sec/batch\n",
      "Epoch: 16/100...  Training Step: 4550...  Training loss: 1.8026...  0.1330 sec/batch\n",
      "Epoch: 16/100...  Training Step: 4600...  Training loss: 1.8362...  0.1020 sec/batch\n",
      "Epoch: 16/100...  Training Step: 4650...  Training loss: 1.8344...  0.0650 sec/batch\n",
      "Epoch: 16/100...  Training Step: 4700...  Training loss: 1.8579...  0.1010 sec/batch\n",
      "Epoch: 17/100...  Training Step: 4750...  Training loss: 1.8205...  0.0660 sec/batch\n",
      "Epoch: 17/100...  Training Step: 4800...  Training loss: 1.8753...  0.1000 sec/batch\n",
      "Epoch: 17/100...  Training Step: 4850...  Training loss: 1.7990...  0.0990 sec/batch\n",
      "Epoch: 17/100...  Training Step: 4900...  Training loss: 1.8292...  0.1360 sec/batch\n",
      "Epoch: 17/100...  Training Step: 4950...  Training loss: 1.7733...  0.0660 sec/batch\n",
      "Epoch: 17/100...  Training Step: 5000...  Training loss: 1.8322...  0.0990 sec/batch\n",
      "Epoch: 18/100...  Training Step: 5050...  Training loss: 1.8306...  0.0640 sec/batch\n",
      "Epoch: 18/100...  Training Step: 5100...  Training loss: 1.8211...  0.1000 sec/batch\n",
      "Epoch: 18/100...  Training Step: 5150...  Training loss: 1.7781...  0.1000 sec/batch\n",
      "Epoch: 18/100...  Training Step: 5200...  Training loss: 1.8143...  0.0990 sec/batch\n",
      "Epoch: 18/100...  Training Step: 5250...  Training loss: 1.8084...  0.0980 sec/batch\n",
      "Epoch: 18/100...  Training Step: 5300...  Training loss: 1.7999...  0.1350 sec/batch\n",
      "Epoch: 19/100...  Training Step: 5350...  Training loss: 1.8026...  0.1000 sec/batch\n",
      "Epoch: 19/100...  Training Step: 5400...  Training loss: 1.7903...  0.1000 sec/batch\n",
      "Epoch: 19/100...  Training Step: 5450...  Training loss: 1.8221...  0.0990 sec/batch\n",
      "Epoch: 19/100...  Training Step: 5500...  Training loss: 1.7735...  0.0650 sec/batch\n",
      "Epoch: 19/100...  Training Step: 5550...  Training loss: 1.7361...  0.1010 sec/batch\n",
      "Epoch: 19/100...  Training Step: 5600...  Training loss: 1.7746...  0.1000 sec/batch\n",
      "Epoch: 20/100...  Training Step: 5650...  Training loss: 1.7603...  0.1000 sec/batch\n",
      "Epoch: 20/100...  Training Step: 5700...  Training loss: 1.7634...  0.0990 sec/batch\n",
      "Epoch: 20/100...  Training Step: 5750...  Training loss: 1.7757...  0.0990 sec/batch\n",
      "Epoch: 20/100...  Training Step: 5800...  Training loss: 1.7653...  0.1000 sec/batch\n",
      "Epoch: 20/100...  Training Step: 5850...  Training loss: 1.7507...  0.1020 sec/batch\n",
      "Epoch: 20/100...  Training Step: 5900...  Training loss: 1.8745...  0.0680 sec/batch\n",
      "Epoch: 21/100...  Training Step: 5950...  Training loss: 1.7551...  0.1020 sec/batch\n",
      "Epoch: 21/100...  Training Step: 6000...  Training loss: 1.7811...  0.0990 sec/batch\n",
      "Epoch: 21/100...  Training Step: 6050...  Training loss: 1.7553...  0.0670 sec/batch\n",
      "Epoch: 21/100...  Training Step: 6100...  Training loss: 1.7303...  0.1000 sec/batch\n",
      "Epoch: 21/100...  Training Step: 6150...  Training loss: 1.7376...  0.1020 sec/batch\n",
      "Epoch: 22/100...  Training Step: 6200...  Training loss: 1.7429...  0.0700 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/100...  Training Step: 6250...  Training loss: 1.7125...  0.1000 sec/batch\n",
      "Epoch: 22/100...  Training Step: 6300...  Training loss: 1.7848...  0.0650 sec/batch\n",
      "Epoch: 22/100...  Training Step: 6350...  Training loss: 1.7661...  0.0680 sec/batch\n",
      "Epoch: 22/100...  Training Step: 6400...  Training loss: 1.7362...  0.0990 sec/batch\n",
      "Epoch: 22/100...  Training Step: 6450...  Training loss: 1.7611...  0.0680 sec/batch\n",
      "Epoch: 23/100...  Training Step: 6500...  Training loss: 1.7353...  0.0680 sec/batch\n",
      "Epoch: 23/100...  Training Step: 6550...  Training loss: 1.7652...  0.0990 sec/batch\n",
      "Epoch: 23/100...  Training Step: 6600...  Training loss: 1.7475...  0.0640 sec/batch\n",
      "Epoch: 23/100...  Training Step: 6650...  Training loss: 1.7089...  0.1000 sec/batch\n",
      "Epoch: 23/100...  Training Step: 6700...  Training loss: 1.7114...  0.1000 sec/batch\n",
      "Epoch: 23/100...  Training Step: 6750...  Training loss: 1.7547...  0.1000 sec/batch\n",
      "Epoch: 24/100...  Training Step: 6800...  Training loss: 1.7312...  0.1000 sec/batch\n",
      "Epoch: 24/100...  Training Step: 6850...  Training loss: 1.7369...  0.1000 sec/batch\n",
      "Epoch: 24/100...  Training Step: 6900...  Training loss: 1.6935...  0.0620 sec/batch\n",
      "Epoch: 24/100...  Training Step: 6950...  Training loss: 1.7100...  0.1000 sec/batch\n",
      "Epoch: 24/100...  Training Step: 7000...  Training loss: 1.7321...  0.1000 sec/batch\n",
      "Epoch: 24/100...  Training Step: 7050...  Training loss: 1.7297...  0.1010 sec/batch\n",
      "Epoch: 25/100...  Training Step: 7100...  Training loss: 1.7212...  0.0620 sec/batch\n",
      "Epoch: 25/100...  Training Step: 7150...  Training loss: 1.7310...  0.1340 sec/batch\n",
      "Epoch: 25/100...  Training Step: 7200...  Training loss: 1.6760...  0.0630 sec/batch\n",
      "Epoch: 25/100...  Training Step: 7250...  Training loss: 1.7300...  0.0640 sec/batch\n",
      "Epoch: 25/100...  Training Step: 7300...  Training loss: 1.7119...  0.0630 sec/batch\n",
      "Epoch: 25/100...  Training Step: 7350...  Training loss: 1.6966...  0.0640 sec/batch\n",
      "Epoch: 26/100...  Training Step: 7400...  Training loss: 1.7100...  0.1000 sec/batch\n",
      "Epoch: 26/100...  Training Step: 7450...  Training loss: 1.7588...  0.0990 sec/batch\n",
      "Epoch: 26/100...  Training Step: 7500...  Training loss: 1.6696...  0.1000 sec/batch\n",
      "Epoch: 26/100...  Training Step: 7550...  Training loss: 1.7183...  0.1000 sec/batch\n",
      "Epoch: 26/100...  Training Step: 7600...  Training loss: 1.7085...  0.1380 sec/batch\n",
      "Epoch: 26/100...  Training Step: 7650...  Training loss: 1.7407...  0.1040 sec/batch\n",
      "Epoch: 27/100...  Training Step: 7700...  Training loss: 1.7004...  0.1360 sec/batch\n",
      "Epoch: 27/100...  Training Step: 7750...  Training loss: 1.7660...  0.1010 sec/batch\n",
      "Epoch: 27/100...  Training Step: 7800...  Training loss: 1.6939...  0.0620 sec/batch\n",
      "Epoch: 27/100...  Training Step: 7850...  Training loss: 1.7147...  0.1000 sec/batch\n",
      "Epoch: 27/100...  Training Step: 7900...  Training loss: 1.6588...  0.0620 sec/batch\n",
      "Epoch: 27/100...  Training Step: 7950...  Training loss: 1.7219...  0.0630 sec/batch\n",
      "Epoch: 28/100...  Training Step: 8000...  Training loss: 1.7244...  0.1010 sec/batch\n",
      "Epoch: 28/100...  Training Step: 8050...  Training loss: 1.7066...  0.1010 sec/batch\n",
      "Epoch: 28/100...  Training Step: 8100...  Training loss: 1.6808...  0.1000 sec/batch\n",
      "Epoch: 28/100...  Training Step: 8150...  Training loss: 1.6999...  0.1360 sec/batch\n",
      "Epoch: 28/100...  Training Step: 8200...  Training loss: 1.6802...  0.0990 sec/batch\n",
      "Epoch: 28/100...  Training Step: 8250...  Training loss: 1.7002...  0.1000 sec/batch\n",
      "Epoch: 29/100...  Training Step: 8300...  Training loss: 1.6792...  0.0990 sec/batch\n",
      "Epoch: 29/100...  Training Step: 8350...  Training loss: 1.6704...  0.1000 sec/batch\n",
      "Epoch: 29/100...  Training Step: 8400...  Training loss: 1.7080...  0.1000 sec/batch\n",
      "Epoch: 29/100...  Training Step: 8450...  Training loss: 1.6690...  0.0650 sec/batch\n",
      "Epoch: 29/100...  Training Step: 8500...  Training loss: 1.6194...  0.1130 sec/batch\n",
      "Epoch: 29/100...  Training Step: 8550...  Training loss: 1.6679...  0.0640 sec/batch\n",
      "Epoch: 30/100...  Training Step: 8600...  Training loss: 1.6717...  0.1000 sec/batch\n",
      "Epoch: 30/100...  Training Step: 8650...  Training loss: 1.6774...  0.0990 sec/batch\n",
      "Epoch: 30/100...  Training Step: 8700...  Training loss: 1.6838...  0.0990 sec/batch\n",
      "Epoch: 30/100...  Training Step: 8750...  Training loss: 1.6620...  0.0620 sec/batch\n",
      "Epoch: 30/100...  Training Step: 8800...  Training loss: 1.6764...  0.0990 sec/batch\n",
      "Epoch: 30/100...  Training Step: 8850...  Training loss: 1.7832...  0.1010 sec/batch\n",
      "Epoch: 31/100...  Training Step: 8900...  Training loss: 1.6701...  0.1000 sec/batch\n",
      "Epoch: 31/100...  Training Step: 8950...  Training loss: 1.7271...  0.1000 sec/batch\n",
      "Epoch: 31/100...  Training Step: 9000...  Training loss: 1.6610...  0.1000 sec/batch\n",
      "Epoch: 31/100...  Training Step: 9050...  Training loss: 1.6283...  0.1010 sec/batch\n",
      "Epoch: 31/100...  Training Step: 9100...  Training loss: 1.6375...  0.0970 sec/batch\n",
      "Epoch: 32/100...  Training Step: 9150...  Training loss: 1.6616...  0.0640 sec/batch\n",
      "Epoch: 32/100...  Training Step: 9200...  Training loss: 1.6342...  0.1000 sec/batch\n",
      "Epoch: 32/100...  Training Step: 9250...  Training loss: 1.7141...  0.1010 sec/batch\n",
      "Epoch: 32/100...  Training Step: 9300...  Training loss: 1.6740...  0.1020 sec/batch\n",
      "Epoch: 32/100...  Training Step: 9350...  Training loss: 1.6394...  0.1000 sec/batch\n",
      "Epoch: 32/100...  Training Step: 9400...  Training loss: 1.6738...  0.1360 sec/batch\n",
      "Epoch: 33/100...  Training Step: 9450...  Training loss: 1.6451...  0.1000 sec/batch\n",
      "Epoch: 33/100...  Training Step: 9500...  Training loss: 1.6670...  0.1370 sec/batch\n",
      "Epoch: 33/100...  Training Step: 9550...  Training loss: 1.6778...  0.1000 sec/batch\n",
      "Epoch: 33/100...  Training Step: 9600...  Training loss: 1.6317...  0.1370 sec/batch\n",
      "Epoch: 33/100...  Training Step: 9650...  Training loss: 1.6360...  0.1000 sec/batch\n",
      "Epoch: 33/100...  Training Step: 9700...  Training loss: 1.6739...  0.1380 sec/batch\n",
      "Epoch: 34/100...  Training Step: 9750...  Training loss: 1.6545...  0.1010 sec/batch\n",
      "Epoch: 34/100...  Training Step: 9800...  Training loss: 1.6549...  0.1000 sec/batch\n",
      "Epoch: 34/100...  Training Step: 9850...  Training loss: 1.6197...  0.0990 sec/batch\n",
      "Epoch: 34/100...  Training Step: 9900...  Training loss: 1.6131...  0.0620 sec/batch\n",
      "Epoch: 34/100...  Training Step: 9950...  Training loss: 1.6547...  0.1000 sec/batch\n",
      "Epoch: 34/100...  Training Step: 10000...  Training loss: 1.6631...  0.1370 sec/batch\n",
      "Epoch: 35/100...  Training Step: 10050...  Training loss: 1.6389...  0.0980 sec/batch\n",
      "Epoch: 35/100...  Training Step: 10100...  Training loss: 1.6481...  0.1370 sec/batch\n",
      "Epoch: 35/100...  Training Step: 10150...  Training loss: 1.6177...  0.1360 sec/batch\n",
      "Epoch: 35/100...  Training Step: 10200...  Training loss: 1.6683...  0.0990 sec/batch\n",
      "Epoch: 35/100...  Training Step: 10250...  Training loss: 1.6410...  0.0640 sec/batch\n",
      "Epoch: 35/100...  Training Step: 10300...  Training loss: 1.6277...  0.1000 sec/batch\n",
      "Epoch: 36/100...  Training Step: 10350...  Training loss: 1.6318...  0.0640 sec/batch\n",
      "Epoch: 36/100...  Training Step: 10400...  Training loss: 1.6694...  0.0990 sec/batch\n",
      "Epoch: 36/100...  Training Step: 10450...  Training loss: 1.5988...  0.0980 sec/batch\n",
      "Epoch: 36/100...  Training Step: 10500...  Training loss: 1.6417...  0.1010 sec/batch\n",
      "Epoch: 36/100...  Training Step: 10550...  Training loss: 1.6430...  0.1370 sec/batch\n",
      "Epoch: 36/100...  Training Step: 10600...  Training loss: 1.6963...  0.0620 sec/batch\n",
      "Epoch: 37/100...  Training Step: 10650...  Training loss: 1.6207...  0.1730 sec/batch\n",
      "Epoch: 37/100...  Training Step: 10700...  Training loss: 1.6937...  0.1000 sec/batch\n",
      "Epoch: 37/100...  Training Step: 10750...  Training loss: 1.6304...  0.1370 sec/batch\n",
      "Epoch: 37/100...  Training Step: 10800...  Training loss: 1.6375...  0.1010 sec/batch\n",
      "Epoch: 37/100...  Training Step: 10850...  Training loss: 1.5963...  0.1010 sec/batch\n",
      "Epoch: 37/100...  Training Step: 10900...  Training loss: 1.6567...  0.1010 sec/batch\n",
      "Epoch: 38/100...  Training Step: 10950...  Training loss: 1.6468...  0.1760 sec/batch\n",
      "Epoch: 38/100...  Training Step: 11000...  Training loss: 1.6476...  0.0630 sec/batch\n",
      "Epoch: 38/100...  Training Step: 11050...  Training loss: 1.6228...  0.1010 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/100...  Training Step: 11100...  Training loss: 1.6404...  0.0630 sec/batch\n",
      "Epoch: 38/100...  Training Step: 11150...  Training loss: 1.6306...  0.0640 sec/batch\n",
      "Epoch: 38/100...  Training Step: 11200...  Training loss: 1.6375...  0.0990 sec/batch\n",
      "Epoch: 39/100...  Training Step: 11250...  Training loss: 1.6140...  0.1000 sec/batch\n",
      "Epoch: 39/100...  Training Step: 11300...  Training loss: 1.6231...  0.1000 sec/batch\n",
      "Epoch: 39/100...  Training Step: 11350...  Training loss: 1.6297...  0.0650 sec/batch\n",
      "Epoch: 39/100...  Training Step: 11400...  Training loss: 1.6242...  0.0650 sec/batch\n",
      "Epoch: 39/100...  Training Step: 11450...  Training loss: 1.5800...  0.1000 sec/batch\n",
      "Epoch: 39/100...  Training Step: 11500...  Training loss: 1.6097...  0.0640 sec/batch\n",
      "Epoch: 40/100...  Training Step: 11550...  Training loss: 1.6058...  0.0990 sec/batch\n",
      "Epoch: 40/100...  Training Step: 11600...  Training loss: 1.6293...  0.0990 sec/batch\n",
      "Epoch: 40/100...  Training Step: 11650...  Training loss: 1.6299...  0.1360 sec/batch\n",
      "Epoch: 40/100...  Training Step: 11700...  Training loss: 1.6163...  0.1010 sec/batch\n",
      "Epoch: 40/100...  Training Step: 11750...  Training loss: 1.6155...  0.1010 sec/batch\n",
      "Epoch: 40/100...  Training Step: 11800...  Training loss: 1.7245...  0.0990 sec/batch\n",
      "Epoch: 41/100...  Training Step: 11850...  Training loss: 1.6185...  0.1000 sec/batch\n",
      "Epoch: 41/100...  Training Step: 11900...  Training loss: 1.6632...  0.1000 sec/batch\n",
      "Epoch: 41/100...  Training Step: 11950...  Training loss: 1.6037...  0.1000 sec/batch\n",
      "Epoch: 41/100...  Training Step: 12000...  Training loss: 1.6014...  0.1000 sec/batch\n",
      "Epoch: 41/100...  Training Step: 12050...  Training loss: 1.5979...  0.0630 sec/batch\n",
      "Epoch: 42/100...  Training Step: 12100...  Training loss: 1.6049...  0.1000 sec/batch\n",
      "Epoch: 42/100...  Training Step: 12150...  Training loss: 1.5890...  0.1350 sec/batch\n",
      "Epoch: 42/100...  Training Step: 12200...  Training loss: 1.6379...  0.1010 sec/batch\n",
      "Epoch: 42/100...  Training Step: 12250...  Training loss: 1.6192...  0.1000 sec/batch\n",
      "Epoch: 42/100...  Training Step: 12300...  Training loss: 1.5980...  0.0640 sec/batch\n",
      "Epoch: 42/100...  Training Step: 12350...  Training loss: 1.6182...  0.1000 sec/batch\n",
      "Epoch: 43/100...  Training Step: 12400...  Training loss: 1.6027...  0.1000 sec/batch\n",
      "Epoch: 43/100...  Training Step: 12450...  Training loss: 1.6217...  0.1000 sec/batch\n",
      "Epoch: 43/100...  Training Step: 12500...  Training loss: 1.6156...  0.1000 sec/batch\n",
      "Epoch: 43/100...  Training Step: 12550...  Training loss: 1.5907...  0.0990 sec/batch\n",
      "Epoch: 43/100...  Training Step: 12600...  Training loss: 1.5881...  0.0620 sec/batch\n",
      "Epoch: 43/100...  Training Step: 12650...  Training loss: 1.6185...  0.1380 sec/batch\n",
      "Epoch: 44/100...  Training Step: 12700...  Training loss: 1.5993...  0.1000 sec/batch\n",
      "Epoch: 44/100...  Training Step: 12750...  Training loss: 1.6025...  0.0990 sec/batch\n",
      "Epoch: 44/100...  Training Step: 12800...  Training loss: 1.5859...  0.0620 sec/batch\n",
      "Epoch: 44/100...  Training Step: 12850...  Training loss: 1.5802...  0.0620 sec/batch\n",
      "Epoch: 44/100...  Training Step: 12900...  Training loss: 1.6089...  0.1390 sec/batch\n",
      "Epoch: 44/100...  Training Step: 12950...  Training loss: 1.6237...  0.0630 sec/batch\n",
      "Epoch: 45/100...  Training Step: 13000...  Training loss: 1.5939...  0.0990 sec/batch\n",
      "Epoch: 45/100...  Training Step: 13050...  Training loss: 1.6126...  0.0620 sec/batch\n",
      "Epoch: 45/100...  Training Step: 13100...  Training loss: 1.5915...  0.1000 sec/batch\n",
      "Epoch: 45/100...  Training Step: 13150...  Training loss: 1.6196...  0.0990 sec/batch\n",
      "Epoch: 45/100...  Training Step: 13200...  Training loss: 1.6060...  0.1020 sec/batch\n",
      "Epoch: 45/100...  Training Step: 13250...  Training loss: 1.5900...  0.0620 sec/batch\n",
      "Epoch: 46/100...  Training Step: 13300...  Training loss: 1.5957...  0.0990 sec/batch\n",
      "Epoch: 46/100...  Training Step: 13350...  Training loss: 1.6379...  0.0990 sec/batch\n",
      "Epoch: 46/100...  Training Step: 13400...  Training loss: 1.5489...  0.1000 sec/batch\n",
      "Epoch: 46/100...  Training Step: 13450...  Training loss: 1.6022...  0.1470 sec/batch\n",
      "Epoch: 46/100...  Training Step: 13500...  Training loss: 1.6028...  0.1010 sec/batch\n",
      "Epoch: 46/100...  Training Step: 13550...  Training loss: 1.6312...  0.1000 sec/batch\n",
      "Epoch: 47/100...  Training Step: 13600...  Training loss: 1.5778...  0.1000 sec/batch\n",
      "Epoch: 47/100...  Training Step: 13650...  Training loss: 1.6396...  0.1360 sec/batch\n",
      "Epoch: 47/100...  Training Step: 13700...  Training loss: 1.5864...  0.1380 sec/batch\n",
      "Epoch: 47/100...  Training Step: 13750...  Training loss: 1.6161...  0.1000 sec/batch\n",
      "Epoch: 47/100...  Training Step: 13800...  Training loss: 1.5450...  0.1010 sec/batch\n",
      "Epoch: 47/100...  Training Step: 13850...  Training loss: 1.6146...  0.1370 sec/batch\n",
      "Epoch: 48/100...  Training Step: 13900...  Training loss: 1.6053...  0.1000 sec/batch\n",
      "Epoch: 48/100...  Training Step: 13950...  Training loss: 1.5971...  0.0650 sec/batch\n",
      "Epoch: 48/100...  Training Step: 14000...  Training loss: 1.5944...  0.1330 sec/batch\n",
      "Epoch: 48/100...  Training Step: 14050...  Training loss: 1.6031...  0.1000 sec/batch\n",
      "Epoch: 48/100...  Training Step: 14100...  Training loss: 1.5769...  0.0990 sec/batch\n",
      "Epoch: 48/100...  Training Step: 14150...  Training loss: 1.5914...  0.0990 sec/batch\n",
      "Epoch: 49/100...  Training Step: 14200...  Training loss: 1.5833...  0.1000 sec/batch\n",
      "Epoch: 49/100...  Training Step: 14250...  Training loss: 1.5710...  0.1000 sec/batch\n",
      "Epoch: 49/100...  Training Step: 14300...  Training loss: 1.5938...  0.1010 sec/batch\n",
      "Epoch: 49/100...  Training Step: 14350...  Training loss: 1.5797...  0.0670 sec/batch\n",
      "Epoch: 49/100...  Training Step: 14400...  Training loss: 1.5280...  0.1370 sec/batch\n",
      "Epoch: 49/100...  Training Step: 14450...  Training loss: 1.5654...  0.1000 sec/batch\n",
      "Epoch: 50/100...  Training Step: 14500...  Training loss: 1.5633...  0.1010 sec/batch\n",
      "Epoch: 50/100...  Training Step: 14550...  Training loss: 1.5949...  0.0670 sec/batch\n",
      "Epoch: 50/100...  Training Step: 14600...  Training loss: 1.5872...  0.1000 sec/batch\n",
      "Epoch: 50/100...  Training Step: 14650...  Training loss: 1.5746...  0.0620 sec/batch\n",
      "Epoch: 50/100...  Training Step: 14700...  Training loss: 1.5543...  0.1000 sec/batch\n",
      "Epoch: 50/100...  Training Step: 14750...  Training loss: 1.6978...  0.1770 sec/batch\n",
      "Epoch: 51/100...  Training Step: 14800...  Training loss: 1.5805...  0.1370 sec/batch\n",
      "Epoch: 51/100...  Training Step: 14850...  Training loss: 1.6255...  0.1010 sec/batch\n",
      "Epoch: 51/100...  Training Step: 14900...  Training loss: 1.5797...  0.1010 sec/batch\n",
      "Epoch: 51/100...  Training Step: 14950...  Training loss: 1.5325...  0.1000 sec/batch\n",
      "Epoch: 51/100...  Training Step: 15000...  Training loss: 1.5452...  0.0650 sec/batch\n",
      "Epoch: 52/100...  Training Step: 15050...  Training loss: 1.5812...  0.0640 sec/batch\n",
      "Epoch: 52/100...  Training Step: 15100...  Training loss: 1.5390...  0.1000 sec/batch\n",
      "Epoch: 52/100...  Training Step: 15150...  Training loss: 1.6099...  0.1020 sec/batch\n",
      "Epoch: 52/100...  Training Step: 15200...  Training loss: 1.5901...  0.1370 sec/batch\n",
      "Epoch: 52/100...  Training Step: 15250...  Training loss: 1.5702...  0.1000 sec/batch\n",
      "Epoch: 52/100...  Training Step: 15300...  Training loss: 1.5814...  0.0620 sec/batch\n",
      "Epoch: 53/100...  Training Step: 15350...  Training loss: 1.5675...  0.1000 sec/batch\n",
      "Epoch: 53/100...  Training Step: 15400...  Training loss: 1.5738...  0.1000 sec/batch\n",
      "Epoch: 53/100...  Training Step: 15450...  Training loss: 1.5783...  0.0610 sec/batch\n",
      "Epoch: 53/100...  Training Step: 15500...  Training loss: 1.5680...  0.0630 sec/batch\n",
      "Epoch: 53/100...  Training Step: 15550...  Training loss: 1.5481...  0.0670 sec/batch\n",
      "Epoch: 53/100...  Training Step: 15600...  Training loss: 1.5822...  0.1000 sec/batch\n",
      "Epoch: 54/100...  Training Step: 15650...  Training loss: 1.5634...  0.1000 sec/batch\n",
      "Epoch: 54/100...  Training Step: 15700...  Training loss: 1.5692...  0.0998 sec/batch\n",
      "Epoch: 54/100...  Training Step: 15750...  Training loss: 1.5607...  0.1000 sec/batch\n",
      "Epoch: 54/100...  Training Step: 15800...  Training loss: 1.5433...  0.1000 sec/batch\n",
      "Epoch: 54/100...  Training Step: 15850...  Training loss: 1.5704...  0.1010 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/100...  Training Step: 15900...  Training loss: 1.5999...  0.1010 sec/batch\n",
      "Epoch: 55/100...  Training Step: 15950...  Training loss: 1.5578...  0.0990 sec/batch\n",
      "Epoch: 55/100...  Training Step: 16000...  Training loss: 1.5634...  0.1010 sec/batch\n",
      "Epoch: 55/100...  Training Step: 16050...  Training loss: 1.5474...  0.0990 sec/batch\n",
      "Epoch: 55/100...  Training Step: 16100...  Training loss: 1.5789...  0.1390 sec/batch\n",
      "Epoch: 55/100...  Training Step: 16150...  Training loss: 1.5679...  0.1380 sec/batch\n",
      "Epoch: 55/100...  Training Step: 16200...  Training loss: 1.5480...  0.1360 sec/batch\n",
      "Epoch: 56/100...  Training Step: 16250...  Training loss: 1.5753...  0.1000 sec/batch\n",
      "Epoch: 56/100...  Training Step: 16300...  Training loss: 1.6000...  0.0990 sec/batch\n",
      "Epoch: 56/100...  Training Step: 16350...  Training loss: 1.5252...  0.0620 sec/batch\n",
      "Epoch: 56/100...  Training Step: 16400...  Training loss: 1.5776...  0.1020 sec/batch\n",
      "Epoch: 56/100...  Training Step: 16450...  Training loss: 1.5807...  0.0660 sec/batch\n",
      "Epoch: 56/100...  Training Step: 16500...  Training loss: 1.5958...  0.1010 sec/batch\n",
      "Epoch: 57/100...  Training Step: 16550...  Training loss: 1.5525...  0.0640 sec/batch\n",
      "Epoch: 57/100...  Training Step: 16600...  Training loss: 1.6191...  0.1380 sec/batch\n",
      "Epoch: 57/100...  Training Step: 16650...  Training loss: 1.5699...  0.0990 sec/batch\n",
      "Epoch: 57/100...  Training Step: 16700...  Training loss: 1.5842...  0.1360 sec/batch\n",
      "Epoch: 57/100...  Training Step: 16750...  Training loss: 1.5227...  0.1030 sec/batch\n",
      "Epoch: 57/100...  Training Step: 16800...  Training loss: 1.5819...  0.1000 sec/batch\n",
      "Epoch: 58/100...  Training Step: 16850...  Training loss: 1.5877...  0.0620 sec/batch\n",
      "Epoch: 58/100...  Training Step: 16900...  Training loss: 1.5738...  0.0990 sec/batch\n",
      "Epoch: 58/100...  Training Step: 16950...  Training loss: 1.5568...  0.1010 sec/batch\n",
      "Epoch: 58/100...  Training Step: 17000...  Training loss: 1.5673...  0.0630 sec/batch\n",
      "Epoch: 58/100...  Training Step: 17050...  Training loss: 1.5553...  0.1000 sec/batch\n",
      "Epoch: 58/100...  Training Step: 17100...  Training loss: 1.5534...  0.1000 sec/batch\n",
      "Epoch: 59/100...  Training Step: 17150...  Training loss: 1.5590...  0.1000 sec/batch\n",
      "Epoch: 59/100...  Training Step: 17200...  Training loss: 1.5669...  0.0630 sec/batch\n",
      "Epoch: 59/100...  Training Step: 17250...  Training loss: 1.5882...  0.1370 sec/batch\n",
      "Epoch: 59/100...  Training Step: 17300...  Training loss: 1.5419...  0.0990 sec/batch\n",
      "Epoch: 59/100...  Training Step: 17350...  Training loss: 1.5258...  0.1010 sec/batch\n",
      "Epoch: 59/100...  Training Step: 17400...  Training loss: 1.5421...  0.1000 sec/batch\n",
      "Epoch: 60/100...  Training Step: 17450...  Training loss: 1.5375...  0.0990 sec/batch\n",
      "Epoch: 60/100...  Training Step: 17500...  Training loss: 1.5496...  0.1010 sec/batch\n",
      "Epoch: 60/100...  Training Step: 17550...  Training loss: 1.5683...  0.1000 sec/batch\n",
      "Epoch: 60/100...  Training Step: 17600...  Training loss: 1.5477...  0.1010 sec/batch\n",
      "Epoch: 60/100...  Training Step: 17650...  Training loss: 1.5488...  0.0720 sec/batch\n",
      "Epoch: 60/100...  Training Step: 17700...  Training loss: 1.6700...  0.1010 sec/batch\n",
      "Epoch: 61/100...  Training Step: 17750...  Training loss: 1.5577...  0.1350 sec/batch\n",
      "Epoch: 61/100...  Training Step: 17800...  Training loss: 1.5950...  0.1000 sec/batch\n",
      "Epoch: 61/100...  Training Step: 17850...  Training loss: 1.5428...  0.0640 sec/batch\n",
      "Epoch: 61/100...  Training Step: 17900...  Training loss: 1.5395...  0.1000 sec/batch\n",
      "Epoch: 61/100...  Training Step: 17950...  Training loss: 1.5293...  0.1000 sec/batch\n",
      "Epoch: 62/100...  Training Step: 18000...  Training loss: 1.5490...  0.0990 sec/batch\n",
      "Epoch: 62/100...  Training Step: 18050...  Training loss: 1.5048...  0.0990 sec/batch\n",
      "Epoch: 62/100...  Training Step: 18100...  Training loss: 1.5896...  0.1010 sec/batch\n",
      "Epoch: 62/100...  Training Step: 18150...  Training loss: 1.5602...  0.0620 sec/batch\n",
      "Epoch: 62/100...  Training Step: 18200...  Training loss: 1.5292...  0.1370 sec/batch\n",
      "Epoch: 62/100...  Training Step: 18250...  Training loss: 1.5837...  0.0630 sec/batch\n",
      "Epoch: 63/100...  Training Step: 18300...  Training loss: 1.5453...  0.1010 sec/batch\n",
      "Epoch: 63/100...  Training Step: 18350...  Training loss: 1.5451...  0.1000 sec/batch\n",
      "Epoch: 63/100...  Training Step: 18400...  Training loss: 1.5530...  0.1360 sec/batch\n",
      "Epoch: 63/100...  Training Step: 18450...  Training loss: 1.5360...  0.1000 sec/batch\n",
      "Epoch: 63/100...  Training Step: 18500...  Training loss: 1.5233...  0.1380 sec/batch\n",
      "Epoch: 63/100...  Training Step: 18550...  Training loss: 1.5561...  0.1230 sec/batch\n",
      "Epoch: 64/100...  Training Step: 18600...  Training loss: 1.5436...  0.1000 sec/batch\n",
      "Epoch: 64/100...  Training Step: 18650...  Training loss: 1.5537...  0.0990 sec/batch\n",
      "Epoch: 64/100...  Training Step: 18700...  Training loss: 1.5253...  0.0700 sec/batch\n",
      "Epoch: 64/100...  Training Step: 18750...  Training loss: 1.5302...  0.1010 sec/batch\n",
      "Epoch: 64/100...  Training Step: 18800...  Training loss: 1.5434...  0.1000 sec/batch\n",
      "Epoch: 64/100...  Training Step: 18850...  Training loss: 1.5591...  0.0650 sec/batch\n",
      "Epoch: 65/100...  Training Step: 18900...  Training loss: 1.5408...  0.1000 sec/batch\n",
      "Epoch: 65/100...  Training Step: 18950...  Training loss: 1.5479...  0.1000 sec/batch\n",
      "Epoch: 65/100...  Training Step: 19000...  Training loss: 1.5384...  0.0990 sec/batch\n",
      "Epoch: 65/100...  Training Step: 19050...  Training loss: 1.5557...  0.0690 sec/batch\n",
      "Epoch: 65/100...  Training Step: 19100...  Training loss: 1.5502...  0.1000 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "batch_size = 128\n",
    "num_steps = 50\n",
    "lstm_size = 128\n",
    "num_layers =2 \n",
    "learning_rate =0.001\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.6,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 10000, lstm_size, len(vocab), prime=\"Carter\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
