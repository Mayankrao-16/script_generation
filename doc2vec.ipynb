{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "data_dir = 'episodes'# data directory containing input.txt\n",
    "save_dir = 'episodes' # directory to store models\n",
    "file_list=[\"HP1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy, and french model\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "#import spacy, and french model\n",
    "import spacy\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "#initiate sentences and labels lists\n",
    "sentences = []\n",
    "sentences_label = []\n",
    "\n",
    "#create sentences function:\n",
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"â€¦\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences\n",
    "\n",
    "#create sentences from files\n",
    "for file_name in file_list:\n",
    "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
    "    #read data\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    #create sentences\n",
    "    doc = nlp(data)\n",
    "    sents = create_sentences(doc)\n",
    "    sentences = sentences + sents\n",
    "    \n",
    "#create labels\n",
    "for i in range(np.array(sentences).shape[0]):\n",
    "    sentences_label.append(\"ID\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(data, docLabels, size=300, sample=0.000001, dm=0, hs=1, window=10, min_count=0, workers=8,alpha=0.024, min_alpha=0.024, epoch=15, save_file='./data/doc2vec.w2v') :\n",
    "    startime = time.time()\n",
    "    \n",
    "    print(\"{0} articles loaded for model\".format(len(data)))\n",
    "\n",
    "    it = LabeledLineSentence(data, docLabels)\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=size, sample=sample, dm=dm, window=window, min_count=min_count, workers=workers,alpha=alpha, min_alpha=min_alpha, hs=hs) # use fixed learning rate\n",
    "    model.build_vocab(it)\n",
    "    for epoch in range(epoch):\n",
    "        print(\"Training epoch {}\".format(epoch + 1))\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        # model.alpha -= 0.002 # decrease the learning rate\n",
    "        # model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "        \n",
    "    #saving the created model\n",
    "    model.save(os.path.join(save_file))\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600 articles loaded for model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mayan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "c:\\users\\mayan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mayan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(sentences, sentences_label, size=500,sample=0.0,alpha=0.025, min_alpha=0.001, min_count=0, window=10, epoch=20, dm=0, hs=1, save_file='episodes\\\\doc2vec.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : ['the', 'sorting', 'hat', 'the', 'door', 'swung', 'open', 'at', 'once', '.']\n",
      "***\n",
      "sentence 500 : ['\"', 'this', 'was', 'so', 'unfair', 'that', 'harry', 'opened', 'his', 'mouth', 'to', 'argue', ',', 'but', 'ron', 'kicked', 'him', 'behind', 'their', 'cauldron', '.']\n",
      "***\n",
      "sentence 1000 : ['\"', 'ron', 'grinned', 'at', 'harry', '.']\n",
      "***\n",
      "sentence 1500 : ['\"', 'dunno', 'what', 'harry', 'thinks', 'he', \"'s\", 'doing', ',', '\"', 'hagrid', 'mumbled', '.']\n",
      "***\n",
      "sentence 2000 : ['the', 'happiest', 'man', 'on', 'earth', 'would', 'be', 'able', 'to', 'use', 'the', 'mirror', 'of', 'erised', 'like', 'a', 'normal', 'mirror', ',', 'that', 'is', ',', 'he', 'would', 'look', 'into', 'it', 'and', 'see', 'himself', 'exactly', 'as', 'he', 'is', '.']\n",
      "***\n",
      "sentence 2500 : ['the', 'clock', 'on', 'the', 'wall', 'had', 'just', 'chimed', 'midnight', 'when', 'the', 'portrait', 'hole', 'burst', 'open', '.']\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from six.moves import cPickle\n",
    "\n",
    "#load the model\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('models\\\\doc2vec.w2v')\n",
    "\n",
    "sentences_vector=[]\n",
    "\n",
    "t = 500\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i % t == 0:\n",
    "        print(\"sentence\", i, \":\", sentences[i])\n",
    "        print(\"***\")\n",
    "    sent = sentences[i]\n",
    "    sentences_vector.append(d2v_model.infer_vector(sent, alpha=0.001, min_alpha=0.001, steps=10000))\n",
    "    \n",
    "#save the sentences_vector\n",
    "sentences_vector_file = os.path.join(\"models\", \"sentences_vector_500_a001_ma001_s10000.pkl\")\n",
    "with open(os.path.join(sentences_vector_file), 'wb') as f:\n",
    "    cPickle.dump((sentences_vector), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "with open(\"models\\\\sentences_vector_500_a001_ma001_s10000.pkl\",'rb') as f:\n",
    "    sentences_vector = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sequence:  0\n",
      "   1 th vector for this sequence. Sentence  ID0 (vector dim =  500 )\n",
      "   2 th vector for this sequence. Sentence  ID1 (vector dim =  500 )\n",
      "   3 th vector for this sequence. Sentence  ID2 (vector dim =  500 )\n",
      "   4 th vector for this sequence. Sentence  ID3 (vector dim =  500 )\n",
      "   5 th vector for this sequence. Sentence  ID4 (vector dim =  500 )\n",
      "   6 th vector for this sequence. Sentence  ID5 (vector dim =  500 )\n",
      "   7 th vector for this sequence. Sentence  ID6 (vector dim =  500 )\n",
      "   8 th vector for this sequence. Sentence  ID7 (vector dim =  500 )\n",
      "   9 th vector for this sequence. Sentence  ID8 (vector dim =  500 )\n",
      "   10 th vector for this sequence. Sentence  ID9 (vector dim =  500 )\n",
      "   11 th vector for this sequence. Sentence  ID10 (vector dim =  500 )\n",
      "   12 th vector for this sequence. Sentence  ID11 (vector dim =  500 )\n",
      "   13 th vector for this sequence. Sentence  ID12 (vector dim =  500 )\n",
      "   14 th vector for this sequence. Sentence  ID13 (vector dim =  500 )\n",
      "   15 th vector for this sequence. Sentence  ID14 (vector dim =  500 )\n",
      "  y vector for this sequence  ID15 : (vector dim =  500 )\n",
      "new sequence:  1000\n",
      "   1 th vector for this sequence. Sentence  ID1000 (vector dim =  500 )\n",
      "   2 th vector for this sequence. Sentence  ID1001 (vector dim =  500 )\n",
      "   3 th vector for this sequence. Sentence  ID1002 (vector dim =  500 )\n",
      "   4 th vector for this sequence. Sentence  ID1003 (vector dim =  500 )\n",
      "   5 th vector for this sequence. Sentence  ID1004 (vector dim =  500 )\n",
      "   6 th vector for this sequence. Sentence  ID1005 (vector dim =  500 )\n",
      "   7 th vector for this sequence. Sentence  ID1006 (vector dim =  500 )\n",
      "   8 th vector for this sequence. Sentence  ID1007 (vector dim =  500 )\n",
      "   9 th vector for this sequence. Sentence  ID1008 (vector dim =  500 )\n",
      "   10 th vector for this sequence. Sentence  ID1009 (vector dim =  500 )\n",
      "   11 th vector for this sequence. Sentence  ID1010 (vector dim =  500 )\n",
      "   12 th vector for this sequence. Sentence  ID1011 (vector dim =  500 )\n",
      "   13 th vector for this sequence. Sentence  ID1012 (vector dim =  500 )\n",
      "   14 th vector for this sequence. Sentence  ID1013 (vector dim =  500 )\n",
      "   15 th vector for this sequence. Sentence  ID1014 (vector dim =  500 )\n",
      "  y vector for this sequence  ID1015 : (vector dim =  500 )\n",
      "new sequence:  2000\n",
      "   1 th vector for this sequence. Sentence  ID2000 (vector dim =  500 )\n",
      "   2 th vector for this sequence. Sentence  ID2001 (vector dim =  500 )\n",
      "   3 th vector for this sequence. Sentence  ID2002 (vector dim =  500 )\n",
      "   4 th vector for this sequence. Sentence  ID2003 (vector dim =  500 )\n",
      "   5 th vector for this sequence. Sentence  ID2004 (vector dim =  500 )\n",
      "   6 th vector for this sequence. Sentence  ID2005 (vector dim =  500 )\n",
      "   7 th vector for this sequence. Sentence  ID2006 (vector dim =  500 )\n",
      "   8 th vector for this sequence. Sentence  ID2007 (vector dim =  500 )\n",
      "   9 th vector for this sequence. Sentence  ID2008 (vector dim =  500 )\n",
      "   10 th vector for this sequence. Sentence  ID2009 (vector dim =  500 )\n",
      "   11 th vector for this sequence. Sentence  ID2010 (vector dim =  500 )\n",
      "   12 th vector for this sequence. Sentence  ID2011 (vector dim =  500 )\n",
      "   13 th vector for this sequence. Sentence  ID2012 (vector dim =  500 )\n",
      "   14 th vector for this sequence. Sentence  ID2013 (vector dim =  500 )\n",
      "   15 th vector for this sequence. Sentence  ID2014 (vector dim =  500 )\n",
      "  y vector for this sequence  ID2015 : (vector dim =  500 )\n",
      "(2600, 15, 500) (2600, 500)\n"
     ]
    }
   ],
   "source": [
    "nb_sequenced_sentences = 15\n",
    "vector_dim = 500\n",
    "\n",
    "X_train = np.zeros((len(sentences), nb_sequenced_sentences, vector_dim), dtype=np.float)\n",
    "y_train = np.zeros((len(sentences), vector_dim), dtype=np.float)\n",
    "\n",
    "t = 1000\n",
    "for i in range(len(sentences_label)-nb_sequenced_sentences-1):\n",
    "    if i % t == 0: print(\"new sequence: \", i)\n",
    "    \n",
    "    for k in range(nb_sequenced_sentences):\n",
    "        sent = sentences_label[i+k]\n",
    "        vect = sentences_vector[i+k]\n",
    "        \n",
    "        if i % t == 0:\n",
    "            print(\"  \", k + 1 ,\"th vector for this sequence. Sentence \", sent, \"(vector dim = \", len(vect), \")\")\n",
    "            \n",
    "        for j in range(len(vect)):\n",
    "            X_train[i, k, j] = vect[j]\n",
    "    \n",
    "    senty = sentences_label[i+nb_sequenced_sentences]\n",
    "    vecty = sentences_vector[i+nb_sequenced_sentences]\n",
    "    if i % t == 0: print(\"  y vector for this sequence \", senty, \": (vector dim = \", len(vecty), \")\")\n",
    "    for j in range(len(vecty)):\n",
    "        y_train[i, j] = vecty[j]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error, logcosh\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vector_dim):\n",
    "    print('Building LSTM model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vector_dim)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(vector_dim))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='logcosh', optimizer=optimizer, metrics=['acc'])\n",
    "    print('LSTM model built.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "LSTM model built.\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 512 # size of RNN\n",
    "vector_dim = 500\n",
    "learning_rate = 0.0001 #learning rate\n",
    "\n",
    "model_sequence = bidirectional_lstm_model(nb_sequenced_sentences, vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\mayan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 2340 samples, validate on 260 samples\n",
      "Epoch 1/40\n",
      "2340/2340 [==============================] - 5s 2ms/step - loss: 0.0622 - acc: 0.0256 - val_loss: 0.0531 - val_acc: 0.0731\n",
      "Epoch 2/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0573 - acc: 0.0603 - val_loss: 0.0527 - val_acc: 0.0731\n",
      "Epoch 3/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0563 - acc: 0.0624 - val_loss: 0.0525 - val_acc: 0.0615\n",
      "Epoch 4/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0557 - acc: 0.0637 - val_loss: 0.0524 - val_acc: 0.0577\n",
      "Epoch 5/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0553 - acc: 0.0607 - val_loss: 0.0523 - val_acc: 0.0538\n",
      "\n",
      "Epoch 00005: saving model to models\\my_model_sequence_lstm.05.hdf5\n",
      "Epoch 6/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0550 - acc: 0.0598 - val_loss: 0.0523 - val_acc: 0.0538\n",
      "Epoch 7/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0547 - acc: 0.0573 - val_loss: 0.0523 - val_acc: 0.0538\n",
      "Epoch 8/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0544 - acc: 0.0624 - val_loss: 0.0522 - val_acc: 0.0538\n",
      "Epoch 9/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0542 - acc: 0.0624 - val_loss: 0.0522 - val_acc: 0.0500\n",
      "Epoch 10/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0540 - acc: 0.0620 - val_loss: 0.0522 - val_acc: 0.0500\n",
      "\n",
      "Epoch 00010: saving model to models\\my_model_sequence_lstm.10.hdf5\n",
      "Epoch 11/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0537 - acc: 0.0607 - val_loss: 0.0522 - val_acc: 0.0577\n",
      "Epoch 12/40\n",
      "2340/2340 [==============================] - 4s 2ms/step - loss: 0.0535 - acc: 0.0607 - val_loss: 0.0522 - val_acc: 0.0577\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30 # minibatch size\n",
    "\n",
    "callbacks=[EarlyStopping(patience=3, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath='models\\\\my_model_sequence_lstm.{epoch:02d}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=5)]\n",
    "\n",
    "history = model_sequence.fit(X_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=40,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "#save the model\n",
    "model_sequence.save('models\\\\my_model_sequence_lstm.final2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
