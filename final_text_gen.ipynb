{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models' # directory to store models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "import spacy\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading doc2Vec model...\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "#load the doc2vec model\n",
    "print(\"loading doc2Vec model...\")\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('models\\\\doc2vec.w2v')\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n",
      "vocabulary loaded !\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(\"vocabulary loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mayan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word prediction model...\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From c:\\users\\mayan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "model loaded!\n",
      "loading sentence selection model...\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the keras models\n",
    "print(\"loading word prediction model...\")\n",
    "model = load_model(save_dir + \"\\\\\" + 'my_model_gen_sentences_lstm.final.hdf5')\n",
    "print(\"model loaded!\")\n",
    "print(\"loading sentence selection model...\")\n",
    "model_sequence = load_model(save_dir + \"\\\\\" + 'my_model_sequence_lstm.final2.hdf5')\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seed(seed_sentences,nb_words_in_seq=20, verbose=False):\n",
    "    #initiate sentences\n",
    "    generated = ''\n",
    "    sentence = []\n",
    "    \n",
    "    #fill the sentence with a default word\n",
    "    for i in range (nb_words_in_seq):\n",
    "        sentence.append(\"le\")\n",
    "    import nltk\n",
    "    from nltk import word_tokenize\n",
    "\n",
    "    seed = word_tokenize(seed_sentences)\n",
    "    \n",
    "    if verbose == True : print(\"seed: \",seed)\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[nb_words_in_seq-i-1]=seed[len(seed)-i-1]\n",
    "        #print(i, sentence)\n",
    "\n",
    "    generated  = untokenize(sentence)\n",
    "    \n",
    "    if verbose == True : print('Generating text with the following seed: \\n\"' + generated + '\"')\n",
    "\n",
    "    return [generated, sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase(sentence, max_words = 50, nb_words_in_seq=20, temperature=1, verbose = False):\n",
    "    generated = \"\"\n",
    "    words_number = max_words - 1\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    seq_length = nb_words_in_seq\n",
    "    #sentence = []\n",
    "    is_punct = False\n",
    "    \n",
    "    for i in range(words_number):\n",
    "        #create the vector\n",
    "        x = np.zeros((1, seq_length, vocab_size))\n",
    "        for t, word in enumerate(sentence):\n",
    "            #print(t, word, vocab[word])\n",
    "            x[0, nb_words_in_seq-len(sentence)+t, vocab[word]] = 1.\n",
    "        #print(x.shape)\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = vocabulary_inv[next_index]\n",
    "        \n",
    "        if verbose == True:\n",
    "            predv = np.array(preds)\n",
    "            #arr = np.array([1, 3, 2, 4, 5])\n",
    "            wi = predv.argsort()[-3:][::-1]\n",
    "            print(\"potential next words: \", vocabulary_inv[wi[0]], vocabulary_inv[wi[1]], vocabulary_inv[wi[2]])\n",
    "\n",
    "        if is_punct == False:\n",
    "            if next_word in ponctuation:\n",
    "                is_punct = True\n",
    "            generated += \" \" + next_word\n",
    "            sentence = sentence[1:] + [next_word]\n",
    "\n",
    "    return(generated, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_phrases_candidates(sentence, max_words = 50,\\\n",
    "                              nb_words_in_seq=20, \\\n",
    "                              temperature=1, \\\n",
    "                              nb_candidates_sents=10, \\\n",
    "                              verbose = False):\n",
    "    phrase_candidate = []\n",
    "    generated_sentence = \"\"\n",
    "    for i in range(nb_candidates_sents):\n",
    "        generated_sentence, new_sentence = generate_phrase(sentence, \\\n",
    "                                                           max_words = max_words, \\\n",
    "                                                           nb_words_in_seq = nb_words_in_seq, \\\n",
    "                                                           temperature=temperature, \\\n",
    "                                                           verbose = False)\n",
    "        phrase_candidate.append([generated_sentence, new_sentence])\n",
    "    \n",
    "    if verbose == True :\n",
    "        for phrase in phrase_candidate:\n",
    "            print(\"   \" , phrase[0])\n",
    "    return phrase_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_vector(sentences_list, verbose = False):\n",
    "    if verbose == True : print(\"generate vectors for each sentence...\")\n",
    "    seq = []\n",
    "    V = []\n",
    "\n",
    "    for s in sentences_list:\n",
    "        v = d2v_model.infer_vector(create_sentences(nlp(s))[0], alpha=0.001, min_alpha=0.001, steps=10000)\n",
    "        V.append(v)\n",
    "    V_val=np.array(V)\n",
    "    V_val = np.expand_dims(V_val, axis=0)\n",
    "    if verbose == True : print(\"Vectors generated!\")\n",
    "    return V_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_phrase(model, V_val, candidate_list, verbose=False):\n",
    "    sims_list = []\n",
    "    preds = model.predict(V_val, verbose=0)[0]\n",
    "    \n",
    "    #calculate vector for each candidate\n",
    "    for candidate in candidate_list:\n",
    "        #calculate vector\n",
    "        #print(\"calculate vector for : \", candidate[1])\n",
    "        V = np.array(d2v_model.infer_vector(candidate[1]))\n",
    "        sim = scipy.spatial.distance.cosine(V,preds)\n",
    "        sims_list.append(sim)\n",
    "    \n",
    "    m = max(sims_list)\n",
    "    index_max = sims_list.index(m)\n",
    "    \n",
    "    if verbose == True :\n",
    "        print(\"selected phrase :\")\n",
    "        print(\"     \", candidate_list[index_max][0])\n",
    "    return candidate_list[index_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paragraphe(phrase_seed, sentences_seed, \\\n",
    "                        max_words = 50, \\\n",
    "                        nb_words_in_seq=20, \\\n",
    "                        temperature=1, \\\n",
    "                        nb_phrases=30, \\\n",
    "                        nb_candidates_sents=10, \\\n",
    "                        verbose=True):\n",
    "    \n",
    "    sentences_list = sentences_seed\n",
    "    sentence = phrase_seed   \n",
    "    text = []\n",
    "    \n",
    "    for p in range(nb_phrases):\n",
    "        if verbose == True : print(\"\")\n",
    "        if verbose == True : print(\"#############\")\n",
    "        print(\"phrase \",p+1, \"/\", nb_phrases)\n",
    "        if verbose == True : print(\"#############\")       \n",
    "        if verbose == True:\n",
    "            print('Sentence to generate phrase : ')\n",
    "            print(\"     \", sentence)\n",
    "            print(\"\")\n",
    "            print('List of sentences to constrain next phrase : ')\n",
    "            print(\"     \", sentences_list)\n",
    "            print(\"\")\n",
    "    \n",
    "        #generate seed training vector\n",
    "        V_val = generate_training_vector(sentences_list, verbose = verbose)\n",
    "\n",
    "        #generate phrase candidate\n",
    "        if verbose == True : print(\"generate phrases candidates...\")\n",
    "        phrases_candidates = define_phrases_candidates(sentence, \\\n",
    "                                                       max_words = max_words, \\\n",
    "                                                       nb_words_in_seq = nb_words_in_seq, \\\n",
    "                                                       temperature=temperature, \\\n",
    "                                                       nb_candidates_sents=nb_candidates_sents, \\\n",
    "                                                       verbose = verbose)\n",
    "        \n",
    "        if verbose == True : print(\"select next phrase...\")\n",
    "        next_phrase = select_next_phrase(model_sequence, \\\n",
    "                                         V_val,\n",
    "                                         phrases_candidates, \\\n",
    "                                         verbose=verbose)\n",
    "        \n",
    "        print(\"Next phrase: \",next_phrase[0])\n",
    "        if verbose == True :\n",
    "            print(\"\")\n",
    "            print(\"Shift phrases in sentences list...\")\n",
    "        for i in range(len(sentences_list)-1):\n",
    "            sentences_list[i]=sentences_list[i+1]\n",
    "\n",
    "        sentences_list[len(sentences_list)-1] = next_phrase[0]\n",
    "        \n",
    "        if verbose == True:\n",
    "            print(\"done.\")\n",
    "            print(\"new list of sentences :\")\n",
    "            print(\"     \", sentences_list)     \n",
    "        sentence = next_phrase[1]\n",
    "        \n",
    "        text.append(next_phrase[0])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"The door swung open at once.\"\n",
    "s2 = \"A tall, black-haired witch in emerald-green robes stood there.\"\n",
    "s3 = \"She had a very stern face and Harry's first thought was that this was not someone to cross.\"\n",
    "s4 = \"They followed Professor McGonagall across the flagged stone floor.\"\n",
    "s5 = \"The Sorting is a very important ceremony because, while you are here, your House will be something like your family within Hogwarts.\"\n",
    "s6 = \"You will have classes with the rest of your House, sleep in your House dormitory, and spend free time in your House common room.\"\n",
    "s7 = \"At the end of the year, the House with the most points is awarded the House cup, a great honor.\"\n",
    "s8 = \"I hope each of you will be a credit to whichever House becomes yours.\"\n",
    "s9 = \"Harry nervously tried to flatten his hair.\"\n",
    "s10 = \"Harry swallowed.\"\n",
    "s11 = \"He looked around anxiously and saw that everyone else looked terrified, too.\"\n",
    "s12 = \"Harry tried hard not to listen to her.\"\n",
    "s13 = \"He kept his eyes fixed on the door.\"\n",
    "s14 = \"One by one, the ghosts floated away through the opposite wall.\"\n",
    "s15 = \"The hundreds of faces staring at them looked like pale lanterns in the flickering candlelight.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The door swung open at once.', 'A tall, black-haired witch in emerald-green robes stood there.', \"She had a very stern face and Harry's first thought was that this was not someone to cross.\", 'They followed Professor McGonagall across the flagged stone floor.', 'The Sorting is a very important ceremony because, while you are here, your House will be something like your family within Hogwarts.', 'You will have classes with the rest of your House, sleep in your House dormitory, and spend free time in your House common room.', 'At the end of the year, the House with the most points is awarded the House cup, a great honor.', 'I hope each of you will be a credit to whichever House becomes yours.', 'Harry nervously tried to flatten his hair.', 'Harry swallowed.', 'He looked around anxiously and saw that everyone else looked terrified, too.', 'Harry tried hard not to listen to her.', 'He kept his eyes fixed on the door.', 'One by one, the ghosts floated away through the opposite wall.', 'The hundreds of faces staring at them looked like pale lanterns in the flickering candlelight.']\n"
     ]
    }
   ],
   "source": [
    "sentences_list = [s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13,s14,s15]\n",
    "print(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the opposite wall. The hundreds of faces staring at them looked like pale lanterns in the flickering candlelight.\n",
      "['the', 'opposite', 'wall', '.', 'The', 'hundreds', 'of', 'faces', 'staring', 'at', 'them', 'looked', 'like', 'pale', 'lanterns', 'in', 'the', 'flickering', 'candlelight', '.']\n"
     ]
    }
   ],
   "source": [
    "phrase_seed, sentences_seed = create_seed(s1 + \" \" + s2 + \" \" +\\\n",
    "                                          s3 + \" \" + s4+ \" \" + s5 + \" \" +\\\n",
    "                                          s6 + \" \" + s7 + \" \" + s8 + \" \" +\\\n",
    "                                          s9+ \" \" + s10 + \" \" + s11 + \" \" +\\\n",
    "                                          s12 + \" \" + s13 + \" \" + s14+ \" \" + s15,20)\n",
    "print(phrase_seed)\n",
    "print(sentences_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase  1 / 10\n",
      "Next phrase:   the own , they had been against the floor .\n",
      "phrase  2 / 10\n",
      "Next phrase:   \" it 's your team , and , you 'll be a enough , potter , \" i 'm yeh ?\n",
      "phrase  3 / 10\n",
      "Next phrase:   \" \" peeves , \" said hagrid , \" said hagrid , and ron , but they knew it was n't think they 'd been an in a house .\n",
      "phrase  4 / 10\n",
      "Next phrase:   \" i 've got to hear , \" said ron , but harry looked at the ground and o ' done it , and you 're going to get past his and your there .\n",
      "phrase  5 / 10\n",
      "Next phrase:   \" snape 's , \" said ron , but he had been the out of the hat , and the solid of the hat , and harry was trying to the invisibility cloak , and that was as they were never on the end of the library , and harry was only to stop him .\n",
      "phrase  6 / 10\n",
      "Next phrase:   madam hooch was trying to be went to a distance .\n",
      "phrase  7 / 10\n",
      "Next phrase:   it was a castle , which was n't like a fifty .\n",
      "phrase  8 / 10\n",
      "Next phrase:   \" \" what ?\n",
      "phrase  9 / 10\n",
      "Next phrase:   \" \" said ron , but ron looked at once , and she had been place in the dursleys ' , but they were trying to be soon .\n",
      "phrase  10 / 10\n",
      "Next phrase:   it was n't be down .\n"
     ]
    }
   ],
   "source": [
    "text = generate_paragraphe([i.lower() for i in sentences_seed], sentences_list, \\\n",
    "                           max_words = 80, \\\n",
    "                           nb_words_in_seq = 30,\\\n",
    "                           temperature=0.25, \\\n",
    "                           nb_phrases=10, \\\n",
    "                           nb_candidates_sents=7, \\\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text: \n",
      " the black of the way , and the standing , they were n't going to be with the other .\n",
      " \" i 'm you ?\n",
      " \" \" i 'm you , \" said ron , but he was sure it was n't going to be making , but they were n't be see that was nothing , but he had been not to be later , but they were n't have been points to get out of the mirror .\n",
      " they were n't be coming from the bludgers , \" said ron .\n",
      " \" what 's you ?\n"
     ]
    }
   ],
   "source": [
    "print(\"generated text: \")\n",
    "for t in text:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
